{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle \n",
    "import nltk\n",
    "#import heapq\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.datasets import load_files\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "\n",
    "from os import listdir\n",
    "from nltk.corpus import stopwords\n",
    "from pickle import dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = pd.read_csv('data1.csv')\n",
    "\n",
    "#Data= Data[Data['Polarity'] != 'neutral']\n",
    "#Data= Data[Data['Polarity'] != 'conflict']\n",
    "#z=Data['Polarity'].values.astype('str')\n",
    "\n",
    "X=Data.iloc[:,0].values\n",
    "y=Data.iloc[:,1].values\n",
    "\n",
    "\n",
    "for data in range(len(y)):\n",
    "    if(y[data]=='positive'):\n",
    "        y[data]=1\n",
    "    elif(y[data]=='negative'):\n",
    "        y[data]=0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stopwords-bn.txt', 'r', encoding='utf8') as bn:\n",
    "    bangla_stop_words = [line.strip() for line in bn]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX=[]\n",
    "for i in range(0, len(X)):\n",
    "    Data = re.sub(r'\\W', ' ', str(X[i]))\n",
    "    #review = review.lower()\n",
    "    Data = re.sub(r'^br$', ' ', Data)\n",
    "    Data = re.sub(r'\\s+br\\s+',' ',Data)\n",
    "    Data = re.sub(r'\\s+[a-z]\\s+', ' ',Data)\n",
    "    Data = re.sub(r'^b\\s+', '', Data)\n",
    "    Data = ' '.join([word for word in Data.split() if word not in bangla_stop_words])  \n",
    "    Data = re.sub(r'\\s+', ' ', Data)\n",
    "    Data = re.sub(r'\\॥’...-!?:-:-‘’/‘‘’।,', ' ', Data)\n",
    "    XX.append(Data)\n",
    "\n",
    "X=XX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: train.pkl\n",
      "Saved: test.pkl\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "trainX, testX, trainy, testY = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
    "\n",
    "# save a dataset to file\n",
    "def save_dataset(dataset, filename):\n",
    "\tdump(dataset, open(filename, 'wb'))\n",
    "\tprint('Saved: %s' % filename)\n",
    "\n",
    "save_dataset([trainX,trainy], 'train.pkl')\n",
    "save_dataset([testX,testY], 'test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.merge import concatenate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filename):\n",
    "\treturn load(open(filename, 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(lines):\n",
    "\ttokenizer = Tokenizer()\n",
    "\ttokenizer.fit_on_texts(lines)\n",
    "\treturn tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(lines):\n",
    "\treturn max([len(s.split()) for s in lines])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(tokenizer, lines, length):\n",
    "\t# integer encode\n",
    "\tencoded = tokenizer.texts_to_sequences(lines)\n",
    "\t# pad encoded sequences\n",
    "\tpadded = pad_sequences(encoded, maxlen=length, padding='post')\n",
    "\treturn padded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max document length: 99\n",
      "Vocabulary size: 13249\n",
      "(4004, 99)\n"
     ]
    }
   ],
   "source": [
    "def define_model(length, vocab_size):\n",
    "\t# channel 1\n",
    "\tinputs1 = Input(shape=(length,))\n",
    "\tembedding1 = Embedding(vocab_size, 100)(inputs1)\n",
    "\tconv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\n",
    "\tdrop1 = Dropout(0.6)(conv1)\n",
    "\tpool1 = MaxPooling1D(pool_size=2)(drop1)\n",
    "\tflat1 = Flatten()(pool1)\n",
    "\t# channel 2\n",
    "\tinputs2 = Input(shape=(length,))\n",
    "\tembedding2 = Embedding(vocab_size, 100)(inputs2)\n",
    "\tconv2 = Conv1D(filters=32, kernel_size=3, activation='relu')(embedding2)\n",
    "\tdrop2 = Dropout(0.6)(conv2)\n",
    "\tpool2 = MaxPooling1D(pool_size=2)(drop2)\n",
    "\tflat2 = Flatten()(pool2)\n",
    "\t# channel 3\n",
    "\tinputs3 = Input(shape=(length,))\n",
    "\tembedding3 = Embedding(vocab_size, 100)(inputs3)\n",
    "\tconv3 = Conv1D(filters=32, kernel_size=5, activation='relu')(embedding3)\n",
    "\tdrop3 = Dropout(0.6)(conv3)\n",
    "\tpool3 = MaxPooling1D(pool_size=2)(drop3)\n",
    "\tflat3 = Flatten()(pool3)\n",
    "\t# merge\n",
    "\tmerged = concatenate([flat1, flat2, flat3])\n",
    "\t# interpretation\n",
    "\tdense1 = Dense(10, activation='relu')(merged)\n",
    "\toutputs = Dense(1, activation='sigmoid')(dense1)\n",
    "\tmodel = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
    "\t# compile\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\t# summarize\n",
    "\tprint(model.summary())\n",
    "\tplot_model(model, show_shapes=True, to_file='multichannel.png')\n",
    "\treturn model\n",
    "\n",
    "# load training dataset\n",
    "trainLines, trainLabels = load_dataset('train.pkl')\n",
    "# create tokenizer\n",
    "tokenizer = create_tokenizer(trainLines)\n",
    "# calculate max document length\n",
    "length = max_length(trainLines)\n",
    "# calculate vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Max document length: %d' % length)\n",
    "print('Vocabulary size: %d' % vocab_size)\n",
    "# encode data\n",
    "trainX = encode_text(tokenizer, trainLines, length)\n",
    "print(trainX.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Asif\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\Asif\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 99)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 99)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 99)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 99, 100)      1324900     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 99, 100)      1324900     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 99, 100)      1324900     input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 96, 32)       12832       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 97, 32)       9632        embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 95, 32)       16032       embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 96, 32)       0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 97, 32)       0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 95, 32)       0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 48, 32)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 48, 32)       0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 47, 32)       0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 1536)         0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 1536)         0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 1504)         0           max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 4576)         0           flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           45770       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            11          dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 4,058,977\n",
      "Trainable params: 4,058,977\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From C:\\Users\\Asif\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "4004/4004 [==============================] - 15s 4ms/step - loss: 0.6903 - acc: 0.5312\n",
      "Epoch 2/10\n",
      "4004/4004 [==============================] - 11s 3ms/step - loss: 0.6330 - acc: 0.6576\n",
      "Epoch 3/10\n",
      "4004/4004 [==============================] - 11s 3ms/step - loss: 0.3752 - acc: 0.8704\n",
      "Epoch 4/10\n",
      "4004/4004 [==============================] - 11s 3ms/step - loss: 0.1473 - acc: 0.9518\n",
      "Epoch 5/10\n",
      "4004/4004 [==============================] - 12s 3ms/step - loss: 0.0631 - acc: 0.9843\n",
      "Epoch 6/10\n",
      "4004/4004 [==============================] - 11s 3ms/step - loss: 0.0338 - acc: 0.9908\n",
      "Epoch 7/10\n",
      "4004/4004 [==============================] - 11s 3ms/step - loss: 0.0241 - acc: 0.9925\n",
      "Epoch 8/10\n",
      "4004/4004 [==============================] - 11s 3ms/step - loss: 0.0179 - acc: 0.9950\n",
      "Epoch 9/10\n",
      "4004/4004 [==============================] - 11s 3ms/step - loss: 0.0168 - acc: 0.9950\n",
      "Epoch 10/10\n",
      "4004/4004 [==============================] - 12s 3ms/step - loss: 0.0167 - acc: 0.9943\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = define_model(length, vocab_size)\n",
    "# fit model\n",
    "model.fit([trainX,trainX,trainX], array(trainLabels), epochs=10, batch_size=128)\n",
    "# save the model\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max document length: 99\n",
      "Vocabulary size: 13249\n",
      "(4004, 99) (1001, 99)\n",
      "Train Accuracy: 99.625375\n",
      "Test Accuracy: 79.820180\n"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    " \n",
    "# load a clean dataset\n",
    "def load_dataset(filename):\n",
    "\treturn load(open(filename, 'rb'))\n",
    " \n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "\ttokenizer = Tokenizer()\n",
    "\ttokenizer.fit_on_texts(lines)\n",
    "\treturn tokenizer\n",
    " \n",
    "# calculate the maximum document length\n",
    "def max_length(lines):\n",
    "\treturn max([len(s.split()) for s in lines])\n",
    " \n",
    "# encode a list of lines\n",
    "def encode_text(tokenizer, lines, length):\n",
    "\t# integer encode\n",
    "\tencoded = tokenizer.texts_to_sequences(lines)\n",
    "\t# pad encoded sequences\n",
    "\tpadded = pad_sequences(encoded, maxlen=length, padding='post')\n",
    "\treturn padded\n",
    " \n",
    "# load datasets\n",
    "trainLines, trainLabels = load_dataset('train.pkl')\n",
    "testLines, testLabels = load_dataset('test.pkl')\n",
    " \n",
    "# create tokenizer\n",
    "tokenizer = create_tokenizer(trainLines)\n",
    "# calculate max document length\n",
    "length = max_length(trainLines)\n",
    "# calculate vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Max document length: %d' % length)\n",
    "print('Vocabulary size: %d' % vocab_size)\n",
    "# encode data\n",
    "trainX = encode_text(tokenizer, trainLines, length)\n",
    "testX = encode_text(tokenizer, testLines, length)\n",
    "print(trainX.shape, testX.shape)\n",
    " \n",
    "# load the model\n",
    "model = load_model('model.h5')\n",
    " \n",
    "# evaluate model on training dataset\n",
    "loss, acc = model.evaluate([trainX,trainX,trainX], array(trainLabels), verbose=0)\n",
    "print('Train Accuracy: %f' % (acc*100))\n",
    " \n",
    "# evaluate model on test dataset dataset\n",
    "loss, acc = model.evaluate([testX,testX,testX],array(testLabels), verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
